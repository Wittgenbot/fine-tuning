{"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Set Up","metadata":{}},{"cell_type":"markdown","source":"## Package Installs","metadata":{}},{"cell_type":"code","source":"! pip install -q trl\n! pip install -q peft\n! pip install -q scipy\n! pip install -q accelerate\n! pip install -q bitsandbytes\n! pip install -q transformers\n! pip install -q huggingface_hub\n! pip install -q wandb\n! pip install -q gcsfs==2023.6.0\n! pip install -q fsspec==2023.6.0\n! pip install -q -U datasets","metadata":{"id":"3nNWXXc7ol1n","outputId":"9ec4a3ed-f02f-4052-8256-eb51932fdeae","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, DataCollatorForLanguageModeling\n\nfrom datasets import load_dataset\n\nfrom trl import SFTTrainer\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n\nimport wandb\nimport huggingface_hub\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"garbage_collection_threshold:0.6,max_split_size_mb:512\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Secrets","metadata":{}},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nWANDB_KEY = user_secrets.get_secret(\"WANDB_KEY\")\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Third-Party Services","metadata":{}},{"cell_type":"markdown","source":"### Weights and Biases","metadata":{}},{"cell_type":"code","source":"wandb.login(key=WANDB_KEY)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hugging Face","metadata":{}},{"cell_type":"code","source":"huggingface_hub.login(token=HF_TOKEN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"id":"3NVvNhohkvwF"}},{"cell_type":"markdown","source":"## Configuration","metadata":{}},{"cell_type":"code","source":"model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)","metadata":{"id":"kvvLg99Opw5R","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Model","metadata":{}},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\ntokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"markdown","source":"## Generate Prompt Function for Given Dataset Point","metadata":{}},{"cell_type":"code","source":"def generate_prompt(data_point):\n    \n    prompt = f\"\"\"<s>\n    [INST] You are a philosophy professor specializing in Ludwig Wittgenstein. Your task is to generate an appropriate response to a philosophy student's question about Ludwig Wittgenstein's philosophy given in square brackets to clarify his/her confusion.\n    Your answer should be accurate, detailed, thorough and relevant. Your tone should be coherent and conversational.\n    [{data_point[\"question\"]}] [/INST]\n    \n    {data_point[\"answer\"]}</s>\"\"\".strip()\n    \n    return prompt","metadata":{"id":"Mjgn9ptNTrw8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the Dataset","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"descartesevildemon/Ludwig-Wittgenstein-QA-Pairs\", split=\"train\")\ndataset","metadata":{"id":"cDH2h3FNmNiD","outputId":"80cbd735-5641-41b3-934b-893e296e5008","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = dataset.to_pandas()\ndf.head(10)","metadata":{"id":"NAP-jYBjrwUc","outputId":"f832b61b-b92f-4b46-bf98-a35336e5aac7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Add \"Prompt\" Column to Dataset","metadata":{}},{"cell_type":"code","source":"text_column = [generate_prompt(data_point) for data_point in dataset]\ndataset = dataset.add_column(\"prompt\", text_column)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = dataset.to_pandas()\ndf.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Shuffle and Tokenize Dataset","metadata":{}},{"cell_type":"code","source":"dataset = dataset.shuffle(seed=1234)\ndataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)","metadata":{"id":"810o72N7SI9A","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split Dataset into \"Train\" and \"Test\"","metadata":{"id":"1H_wXxkhC8Yv"}},{"cell_type":"code","source":"# dataset = dataset.train_test_split(test_size=0.1)\n# train_data = dataset[\"train\"]\n# test_data = dataset[\"test\"]\n\ntrain_data = dataset","metadata":{"id":"3TKCLTOVDR1x","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine-Tuning","metadata":{}},{"cell_type":"markdown","source":"## Set Up","metadata":{}},{"cell_type":"code","source":"model.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)","metadata":{"id":"NMELsVV6q2my","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"id":"cm3nXV988zew","outputId":"ebac3c3a-50e6-46f6-d94a-ba3a2ae8d23a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Find All Linear Layers in Model","metadata":{}},{"cell_type":"code","source":"import bitsandbytes as bnb\ndef find_all_linear_names(model):\n  cls = bnb.nn.Linear4bit\n  lora_module_names = set()\n  for name, module in model.named_modules():\n    if isinstance(module, cls):\n      names = name.split('.')\n      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names: # needed for 16-bit\n      lora_module_names.remove('lm_head')\n  return list(lora_module_names)","metadata":{"id":"acCr5AZ0831z","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modules = find_all_linear_names(model)\nprint(modules)","metadata":{"id":"DhtO5dMr9Gq3","outputId":"9e3e70e5-d7df-41a4-ab8e-fa3696d58873","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Running the Fine-Tuning\n\nSupervised fine-tuning using QLoRA","metadata":{"id":"PJxy4y9Owe4z"}},{"cell_type":"markdown","source":"### Fine-Tuning Parameters","metadata":{}},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=modules,\n    lora_dropout=0.075,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)","metadata":{"id":"glEtbT3z_hme","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_peft_model(model, lora_config)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    run_name=\"Mistral-7b-Instruct-v0p2 FTing 4EPOCH\",\n    output_dir=\"/kaggle/working/finetune_output\",\n    logging_dir=\"/kaggle/working/finetune_logs\",\n    report_to=\"wandb\",\n    num_train_epochs=4,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n#     warmup_ratio=0.03,\n    learning_rate=2e-4,\n    weight_decay=1e-4,\n    optim=\"paged_adamw_8bit\",\n    fp16=True,\n    logging_steps=20,\n    save_strategy=\"epoch\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\ntorch.cuda.empty_cache()\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_data,\n#     eval_dataset=test_data,\n    dataset_text_field=\"prompt\",\n    peft_config=lora_config,\n    args=training_args,\n    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)","metadata":{"id":"pQyMqLg5izHF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Starting Training Process","metadata":{"id":"BXdeRUxUwhHk"}},{"cell_type":"code","source":"trainable, total = model.get_nb_trainable_parameters()\nprint(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")","metadata":{"id":"LIWgYbz9C2ee","outputId":"ed737fe5-9a16-417e-e4f6-815a1a1e3333","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.config.use_cache = False\n\ntrainer.train()","metadata":{"id":"W4HNvrh5FYqM","outputId":"6fcb08ad-ce9d-4ae7-ddfc-3ff4bb29e5a0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Post-Fine-Tuning","metadata":{}},{"cell_type":"markdown","source":"### Saving Fine-Tuned Model","metadata":{}},{"cell_type":"code","source":"new_model = \"wittgenbot-finetune-test\"\ntrainer.model.save_pretrained(new_model)","metadata":{"id":"RJF_XnsUqLaF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cleaning-up GPU Memory before Merging","metadata":{}},{"cell_type":"code","source":"! nvidia-smi","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del [model, tokenizer, lora_config, trainer, train_data, bnb_config, training_args, df]\ndel [TrainingArguments, SFTTrainer, LoraConfig, BitsAndBytesConfig]\n\nfor _ in range(100):\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! nvidia-smi","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Merging Fine-Tuned Model with Base Model (Mistral-7B-Instruct-v0.2)","metadata":{}},{"cell_type":"code","source":"base_model = AutoModelForCausalLM.from_pretrained(model_id,\n                                                  low_cpu_mem_usage=True,\n                                                  return_dict=True,\n                                                  torch_dtype=torch.float16,\n                                                  device_map={\"\": 0})\n\nmerged_model = PeftModel.from_pretrained(base_model, new_model)\nmerged_model = merged_model.merge_and_unload()\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.unk_token\ntokenizer.padding_side = 'right'\ntokenizer.add_eos_token = False\ntokenizer.add_bos_token = False","metadata":{"id":"SpDZTJ5VqQt9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Saving Merged Model & Tokenizer","metadata":{}},{"cell_type":"code","source":"merged_model_path = '/kaggle/working/wittgenbot-merged-model'\n\nmerged_model.save_pretrained(merged_model_path, safe_serialization=True)\ntokenizer.save_pretrained(merged_model_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pushing Merged Model & Tokenizer to Hugging Face","metadata":{}},{"cell_type":"code","source":"repo_id = 'descartesevildemon/Wittgenbot-7B'\n\nmerged_model.push_to_hub(repo_id=repo_id)\ntokenizer.push_to_hub(repo_id=repo_id)","metadata":{"id":"WV1csnOBNEyK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{},"execution_count":null,"outputs":[]}]}